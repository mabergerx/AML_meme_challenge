{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flair_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-cC6foni7Ia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bD1LL7MjlgQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdg8qLeAjGax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_train_data():\n",
        "    raw_train_data = pd.read_csv(\"/content/drive/My Drive/Meme Analysis Challenge/train_data.csv\")\n",
        "    full_df = pd.merge(raw_train_data, pd.read_csv(\"/content/drive/My Drive/Meme Analysis Challenge/train_label.csv\"), on=\"id\")\n",
        "    full_df.fillna(\"No text\", inplace=True)\n",
        "    return full_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SLz4GmDjJYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_df = load_train_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loclP0dNjbJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "\n",
        "    found_url = re.findall(r\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org)))\", text)\n",
        "    if found_url:\n",
        "        text = text.replace(found_url[0], \"\")\n",
        "        text = text.replace(found_url[0].split(\".\")[0], \"\")\n",
        "    for w in text.split():\n",
        "        if \".com\" in w or \"@\" in w:\n",
        "            text = text.replace(w, \"\")\n",
        "            \n",
        "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
        "    text = re.sub(r'^b\\s+', '', text)\n",
        "    # We remove all the single characters\n",
        "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
        "    # For now we remove all punctuation, but maybe for memes stuff like ! and ? important, we can test later.\n",
        "    text = re.sub(r'[^\\w\\s]','', text)\n",
        "\n",
        "    \n",
        "    return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfTZ2VfyjqXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lemmatize_text(text):\n",
        "    # Make a spacy pipe ffrom this later.\n",
        "    doc = nlp(text)\n",
        "    lemmas = [token.lemma_ if token.lemma_ not in [\"-PRON-\"] else token.text for token in doc]\n",
        "    return \" \".join(lemmas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tke8WQKpjsJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def add_relevant_columns(df, lemmatize=True):\n",
        "    \n",
        "    def add_lowercased_texts(df):\n",
        "        df[\"lowercased_text\"] = [t.lower() for t in df.Corrected_text]\n",
        "        return df\n",
        "    \n",
        "    def clean_texts(df):\n",
        "        df[\"cleaned_text\"] = df.lowercased_text.apply(lambda x: clean_text(x))\n",
        "        return df\n",
        "    \n",
        "    def lemmatize_texts(df):\n",
        "        df[\"lemmatized_text\"] = df.cleaned_text.apply(lambda x: lemmatize_text(x))\n",
        "        return df\n",
        "    \n",
        "    if lemmatize:\n",
        "        return lemmatize_texts(clean_texts(add_lowercased_texts(df)))\n",
        "    else:\n",
        "        return clean_texts(add_lowercased_texts(df))\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BmMggc6jt2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prepared_train = add_relevant_columns(full_df, lemmatize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB6oCWYIjvhq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prepared_train.sample(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fADxt_EUkE5x",
        "colab_type": "text"
      },
      "source": [
        "Flair now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7U5F-gwkFyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import TREC_6\n",
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentEmbeddings, DocumentPoolEmbeddings, BertEmbeddings, BytePairEmbeddings, StackedEmbeddings\n",
        "from flair.models import TextClassifier\n",
        "from flair.trainers import ModelTrainer\n",
        "from flair.models import SequenceTagger"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i70VNI4QkQkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import CSVClassificationCorpus\n",
        "import numpy as np\n",
        "from flair.data import Sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "071-IIpiGdvW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# initialize the word embeddings\n",
        "glove_embedding = WordEmbeddings('glove')\n",
        "flair_embedding_forward = FlairEmbeddings('news-forward')\n",
        "flair_embedding_backward = FlairEmbeddings('news-backward')\n",
        "\n",
        "# initialize the document embeddings, take the average of the embeddings\n",
        "document_embeddings = DocumentPoolEmbeddings([glove_embedding,\n",
        "                                              flair_embedding_backward,\n",
        "                                              flair_embedding_forward])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJEUodSFGd5v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [Sentence(s) for s in full_df.Corrected_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zxe7JhcGd-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_embeddings = []\n",
        "for s in sentences:\n",
        "    # embed the sentence with our document embedding\n",
        "    document_embeddings.embed(s)\n",
        "\n",
        "    # now check out the embedded sentence.\n",
        "    embedding = s.get_embedding()\n",
        "    train_embeddings.append(embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPHrZyqtH0zF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_embeddings_np = np.array([t.cpu().detach().numpy() for t in train_embeddings])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJLZL_dbIeTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.save(\"flair_embeddings_train.npy\", train_embeddings_np)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXlumm7FIeQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_embeddings_np.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2lHt--DGeDt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = pd.read_csv(\"/content/drive/My Drive/Meme Analysis Challenge/test_data.csv\")\n",
        "test_data.fillna(\"No text\", inplace=True)\n",
        "test_sentences = [Sentence(s) for s in test_data.Corrected_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNVI5IeQGeIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_embeddings = []\n",
        "for s in test_sentences:\n",
        "    # embed the sentence with our document embedding\n",
        "    document_embeddings.embed(s)\n",
        "\n",
        "    # now check out the embedded sentence.\n",
        "    embedding = s.get_embedding()\n",
        "    test_embeddings.append(embedding.cpu().detach().numpy())\n",
        "\n",
        "test_embeddings_np = np.array(test_embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAGP9gxhHmQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.save(\"flair_embeddings_test.npy\", test_embeddings_np)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KfIzXTNqvr7H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the dataset into 80% train and test and val 10% each.\n",
        "train, validate, test = np.split(prepared_train.sample(frac=1), [int(.6*len(prepared_train)), int(.8*len(prepared_train))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P93baO-SxK6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "validate.to_csv(\"data_folder/dev.csv\")\n",
        "train.to_csv(\"data_folder/train.csv\")\n",
        "test.to_csv(\"data_folder/test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RyQOCWk2DYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_folder = '/content/data_folder'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lvl25ylT2UJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# column format indicating which columns hold the text and label(s)\n",
        "column_name_map = {6: \"text\", 3: \"label\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrMa_MxF31-w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load corpus containing training, test and dev data\n",
        "corpus: Corpus = CSVClassificationCorpus(data_folder,\n",
        "                                         column_name_map,\n",
        "                                         skip_header=True\n",
        ") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H5lu-1657tE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus.filter_empty_sentences()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT71rwsc3_pu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2. create the label dictionary\n",
        "label_dict = corpus.make_label_dictionary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrCkl_694Fa1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_dict.item2idx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0aqmwqM4Vq-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This way we can combine multiple embedding sources for each token in our data. Here we take the BytePair embeddings (ex. \"cat\", \"er\", \"pill\", \"ar\" for the word \"caterpillar\") that helps with embedding unknown tokens, \n",
        "# Flair backward and forward contextualized embeddings and the BERT embeddings.\n",
        "word_embeddings = [\n",
        "                   BytePairEmbeddings('en'), \n",
        "                    FlairEmbeddings('mix-forward'), \n",
        "                    FlairEmbeddings('mix-backward'), \n",
        "                    BertEmbeddings('bert-base-uncased')\n",
        "                   ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcPwizHa452U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4. initialize document embedding by passing list of word embeddings\n",
        "# Can choose between many RNN types (GRU by default)\n",
        "document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(word_embeddings,\n",
        "                                                                     hidden_size=512,\n",
        "                                                                     reproject_words=True,\n",
        "                                                                     reproject_words_dimension=256,\n",
        "                                                                     )\n",
        "\n",
        "# 5. create the text classifier\n",
        "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n",
        "\n",
        "# 6. initialize the text classifier trainer\n",
        "trainer = ModelTrainer(classifier, corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYIVCxzc5JBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 7. start the training\n",
        "trainer.train('/content/drive/My Drive/Meme Analysis Challenge/flair',\n",
        "              learning_rate=0.3,\n",
        "              mini_batch_size=16,\n",
        "              anneal_factor=0.7,\n",
        "              patience=5,\n",
        "              max_epochs=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbe2q55n5LSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classifier = TextClassifier.load('/content/drive/My Drive/Meme Analysis Challenge/flair/final-model.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5OnegC9_Vtj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_sentences = pd.read_csv(\"/content/drive/My Drive/Meme Analysis Challenge/test_data.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wef6OZ__XVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_real_test_set(lemmatize=True):\n",
        "    test_data = pd.read_csv(\"/content/drive/My Drive/Meme Analysis Challenge/test_data.csv\")\n",
        "    test_data.fillna(\"No text\", inplace=True)\n",
        "    test_texts_lowercase = [t.lower() for t in test_data.Corrected_text]\n",
        "    test_data[\"lowercased_text\"] = test_texts_lowercase\n",
        "    test_data[\"cleaned_text\"] = test_data.lowercased_text.apply(lambda x: clean_text(x))\n",
        "    if lemmatize:\n",
        "        test_data[\"lemmatized_text\"] = test_data.cleaned_text.apply(lambda x: lemmatize_text(x))\n",
        "    else:\n",
        "        pass\n",
        "    return test_data\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJFtAkuDA0mW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_test_set = load_real_test_set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmx4rvdSBua2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flair.data import Sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPgdfZUuBJzZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [Sentence(s) for s in real_test_set.lemmatized_text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWKo63_nCElI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_predictions = [classifier.predict(sentence) for sentence in sentences]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoK7rILQm9KX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels_test = [tp[0].to_dict()['labels'][0]['value'] for tp in test_predictions]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DZO3JGgoGVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(labels_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvJB6Yixqowz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_test_set[\"Humour\"] = labels_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEybbQKcrFXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_predictions_df(test_data_with_pred):\n",
        "    df = test_data_with_pred[[\"id\", \"Humour\"]]\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxAsBr9crLku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_df = make_predictions_df(real_test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qNh40BVrOpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_df.to_csv(\"submission_14.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXvgIQXxrZUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}