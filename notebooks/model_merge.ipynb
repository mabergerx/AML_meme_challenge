{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.layers.merge import concatenate, Concatenate\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "import keract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of progressively loading images from file\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "dir_path = 'train_set/'\n",
    "# numbers are based on mode, varified with histogram\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "# create generator\n",
    "datagen = ImageDataGenerator(validation_split=0.2)\n",
    "# prepare an iterators for each dataset\n",
    "train_it = datagen.flow_from_directory(dir_path, \n",
    "                                       batch_size=64,\n",
    "                                       target_size=(img_width, img_height),\n",
    "                                       subset='training')\n",
    "val_it = datagen.flow_from_directory(dir_path,\n",
    "                                     batch_size=64,\n",
    "                                     target_size=(img_width, img_height),\n",
    "                                     subset='validation')\n",
    "# confirm the iterator works\n",
    "batchX, batchy = train_it.next()\n",
    "val_batchX, val_batchy  = val_it.next()\n",
    "print('Batch shape=%s, min=%.3f, max=%.3f' % (batchX.shape, batchX.min(), batchX.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_base_model = VGG16(input_shape=(224, 224, 3), include_top=False, weights='imagenet', classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model = Sequential()\n",
    "image_model.add(image_base_model)\n",
    "image_model.add(Flatten(input_shape=image_base_model.output_shape[1:], name='top_flatter'))\n",
    "image_model.add(Dense(256, activation='relu', name='top_relu', kernel_regularizer=l2(1e-5)))\n",
    "image_model.add(Dropout(0.5))\n",
    "image_model.add(Dense(256, activation='relu', name='top_relu2', kernel_regularizer=l2(1e-5)))\n",
    "image_model.add(Dropout(0.5))\n",
    "image_model.add(Dense(4, activation='sigmoid', name='top_sigmoid', kernel_regularizer=l2(1e-5)))\n",
    "\n",
    "for layer in image_base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "image_model.compile(optimizer=\"adam\", metrics=[\"accuracy\"], loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_model.fit_generator(\n",
    "    train_it,\n",
    "    epochs=5,\n",
    "    validation_data=val_it,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = []\n",
    "for im in []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = np.concatenate((activations['top_relu2/Relu:0'][0], text_features[0].toarray()[0]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer=\"char\", max_df=0.4, max_features=5000, min_df=3, ngram_range=(1, 3), use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = pd.read_csv(\"test_data.csv\").fillna(\"none\").Corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = tfidf.fit_transform(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_predictions = [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.nasnet import preprocess_input, decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = image.load_img(\"test_set/test_1.jpg\", target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(x):\n",
    "    img = image.load_img(x, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_final_model(model, vec_size, n_classes=4, do=0.5, l2_strength=1e-5):\n",
    "    \"\"\"\n",
    "    model : The VGG conv only part with loaded weights\n",
    "    vec_size : The size of the vectorized text vector coming from the bag of words \n",
    "    n_classes : How many classes are you trying to classify ? \n",
    "    do : 0.5 Dropout probability\n",
    "    l2_strenght : The L2 regularization strength\n",
    "    \n",
    "    output : The full model that takes images of size (224, 224) and an additional vector\n",
    "    of size vec_size as input\n",
    "    \"\"\"\n",
    "    \n",
    "    ### top_aux_model takes the vectorized text as input\n",
    "    top_aux_model = Sequential()\n",
    "    top_aux_model.add(Dense(vec_size, input_shape=(vec_size,), name='aux_input'))\n",
    "\n",
    "#     ### top_model takes output from VGG conv and then adds 2 hidden layers\n",
    "#     top_model = Sequential()\n",
    "#     top_model.add(Flatten(input_shape=model.output_shape[1:], name='top_flatter'))\n",
    "#     top_model.add(Dense(256, activation='relu', name='top_relu', W_regularizer=l2(l2_strength)))\n",
    "#     top_model.add(Dropout(do))\n",
    "#     top_model.add(Dense(256, activation='softmax', name='top_softmax', W_regularizer=l2(l2_strength)))\n",
    "#     ### this is than added to the VGG conv-model\n",
    "#     model.add(top_model)\n",
    "    \n",
    "    ### here we merge 'model' that creates features from images with 'top_aux_model'\n",
    "    ### that are the bag of words features extracted from the text. \n",
    "    merged = Concatenate([model, top_aux_model])\n",
    "\n",
    "    ### final_model takes the combined feature vectors and add a sofmax classifier to it\n",
    "    final_model = Sequential()\n",
    "    final_model.add(merged)\n",
    "    final_model.add(Dropout(do))\n",
    "    final_model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel = make_final_model(image_model, 4403)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
