{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "LSTM.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yTn46LdsA15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "from keras.models import Sequential \n",
        "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D, BatchNormalization\n",
        "from keras import regularizers\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYu4_DyIsA19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_path = \"../train_data.csv\"\n",
        "train_label_path = \"../train_label.csv\"\n",
        "test_data_path = \"../test_data.csv\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl3XaOAzsA1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_train_data(train_data_path=train_data_path, train_label_path=train_label_path):\n",
        "    raw_train_data = pd.read_csv(train_data_path)\n",
        "    full_df = pd.merge(raw_train_data, pd.read_csv(train_label_path), on=\"id\")\n",
        "    full_df.fillna(\"No text\", inplace=True)\n",
        "    return full_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8dJoiQSsA2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = load_train_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P13VG-tsA2B",
        "colab_type": "text"
      },
      "source": [
        "### Data is loaded, now off to cleaning.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ1MSOyosA2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "    BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "    STOPWORDS = set(stopwords.words('english'))\n",
        "    text = text.lower() \n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text)\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) \n",
        "    text = text.replace('x', '')\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) \n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75XF_NmosA2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df[\"Corrected_text\"] = df.Corrected_text.apply(lambda x: clean_text(x)).str.replace('\\d+', '')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyRPEH8nsA2F",
        "colab_type": "text"
      },
      "source": [
        "### now we're ready to start tokenize the text and start the data prepping\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hvek_WmLsA2F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_text(MAX_NB_WORDS = 50000, EMBEDDING_DIM = 100, MAX_SEQUENCE_LENGTH = 250):\n",
        "    tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
        "    tokenizer.fit_on_texts(df.Corrected_text.values)\n",
        "    word_index = tokenizer.word_index\n",
        "    \n",
        "    print('Total of %s unique tokens found.' % len(word_index))\n",
        "    X = tokenizer.texts_to_sequences(df.Corrected_text.values)\n",
        "    X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "    Y = pd.get_dummies(df.Humour).values\n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMOliNUDsA2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, Y = tokenize_text()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W-YmreKsA2J",
        "colab_type": "text"
      },
      "source": [
        "### and we're ready to start the splitting and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiSoJV5PsA2J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_split(X=X, Y=Y,  test_size = 0.10, random_state = 42):\n",
        "    \n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = test_size, random_state = random_state)\n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = create_split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "M_z1reatsA2K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initiate_model_traning(X_train=X_train, Y_train=Y_train, epochs=1, batch_size=64, input_length=X.shape[1]):\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
        "    model.add(SpatialDropout1D(0.4))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LSTM(100, dropout=0.4, recurrent_dropout=0.4, kernel_regularizer=regularizers.l2(0.01), activity_regularizer=regularizers.l1(0.01)))\n",
        "    model.add(Dense(4, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6COdJ3_sA2M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_and_train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHgOUyoZsA2N",
        "colab_type": "text"
      },
      "source": [
        "### Done with the training, now let's make our predictions. PS. don't forget to save the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpR4qaSKsA2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_predictions(model=model, test=pd.read_csv(test_data_path), MAX_SEQUENCE_LENGTH=250): \n",
        "    test = test.fillna(\"\")\n",
        "    seq = tokenizer.texts_to_sequences(test.Corrected_text)\n",
        "    padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "    labels = {0:'funny', 1:'hilarious', 2:'not_funny', 3:'very_funny'}\n",
        "    pred = list(model.predict_classes(padded))\n",
        "    pred = np.array([labels[i] for i in pred])\n",
        "    test[\"Humour\"] = pred\n",
        "    test = test.drop([\"Corrected_text\"], axis=1)\n",
        "    test.to_csv(\"predictions_LSTM.csv\", index=False)\n",
        "    print(\"Predictions are made! Ready to upload.\")\n",
        "pred = make_predictions()\n",
        "pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FT0Jr9llsA2P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.save(\"lstm.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}